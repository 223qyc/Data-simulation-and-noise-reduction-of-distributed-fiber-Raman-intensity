# 写在前面
## 目录结构
```
├── 1DCNN
│   ├── evaulate.py
│   └── train.py
├── ADSDN
│   ├── evaulate.py
│   └── train.py
├── APIDN
│   ├── evaulate.py
│   └── train.py
├── DSDN
│   ├── evaulate.py
│   └── train.py
├── PIDN
│   ├── evaulate.py
│   └── train.py
├── README.md
├── RRCDNet
│   ├── evaulate.py
│   └── train.py
├── data
│   ├── test.npz
│   └── train_val.npz
├── 数据查看与图像绘制.py
└── 数据集产生.py
```
## 使用说明
通过主目录下的数据生产程序获得两个数据集，分别是训练-验证集，测试集。

通过数据查看程序从训练-验证集中随机抽取数据进行可视化展示。

各自网络均对应独立目录，包括训练和评估。但是共用同一套数据集以及测试基准。

## 具体细节
train过程将代码保存在当前目录下新建文件夹中，训练结束后讲保存损失图像，训练过程中将建立日志实现训练跟踪，日志加入时间戳且保存在本地，提供后续查看。

evaulate过程不直接展示图像，仅将测试结果在终端输出。图像和测试日志均保存在当前目录下新建文件夹中，图像采用矢量图存储。




---------------------


# 数据仿真
## 1. 干净信号的生成

**原理：**

真实的拉曼光谱在某些波数范围内可能表现出强度相对稳定的特征，形成连续的“平台”。为了模拟这种连续性，我们生成分段的常数信号。

**实现步骤：**

1.  **分段：** 将信号的整个长度随机分割成若干个连续的片段。每个片段的长度在 1 到预设的最大重复次数 (`max_repeat`) 之间随机选取。
2.  **赋值：** 对于每个片段，赋予一个在 0 到 1 之间均匀分布的随机强度值。
3.  **连续性：** 这些片段首尾相连，形成一个具有局部连续性的干净信号。

**数学表示：**

令干净信号为 $S_{clean}[t]$, 其中 $t$ 是时间或波数索引。信号被划分为 $K$ 个连续片段。第 $k$ 个片段的起始位置为 $p_k$，长度为 $l_k$，强度值为 $v_k$。则：

$$
S_{clean}[t] = v_k, \quad \text{对于 } p_k \le t < p_k + l_k
$$

其中，$p_0 = 0$，且 $p_{k+1} = p_k + l_k$，$\sum_{k=0}^{K-1} l_k = signal\_length$。

**归一化：**

生成后，对每个样本的干净信号进行归一化，将其数值范围缩放到 \[0, 1] 区间。这通过以下公式实现：

$$
S_{normalized}[t] = \frac{S_{clean}[t] - \min(S_{clean})}{\max(S_{clean}) - \min(S_{clean}) + \epsilon}
$$

其中，$\epsilon$ 是一个很小的常数，用于防止除以零。

## 2. 添加高斯白噪声

**原理：**

真实拉曼光谱通常会受到随机噪声的干扰。高斯白噪声是一种常见的噪声模型，其特点是在所有频率上具有均匀的功率谱密度，且幅度服从正态分布。信噪比 (SNR) 用于衡量信号强度相对于噪声强度的比例。

**实现步骤：**

1.  **计算信号功率：** 估算干净信号的平均功率 $P_{signal}$。
2.  **随机生成信噪比：** 从预设的信噪比范围 (`snr_range`) 内随机选择一个信噪比值 $SNR_{dB}$（单位为分贝）。
3.  **计算噪声标准差：** 根据信号功率和信噪比，计算高斯噪声的标准差 $\sigma_{noise}$。信噪比的定义如下：

    $$
    SNR_{dB} = 10 \log_{10} \left( \frac{P_{signal}}{P_{noise}} \right) = 10 \log_{10} \left( \frac{P_{signal}}{\sigma_{noise}^2} \right)
    $$

    由此可得噪声标准差：

    $$
    \sigma_{noise} = \sqrt{\frac{P_{signal}}{10^{SNR_{dB}/10}}}
    $$
4.  **生成高斯噪声：** 生成服从均值为 0，标准差为 $\sigma_{noise}$ 的高斯分布的随机噪声 $N(0, \sigma_{noise}^2)[t]$。
5.  **叠加噪声：** 将生成的高斯噪声添加到归一化后的干净信号中，得到带高斯噪声的信号 $S_{noisy\_gaussian}[t]$：

    $$
    S_{noisy\_gaussian}[t] = S_{normalized}[t] + N(0, \sigma_{noise}^2)[t]
    $$

## 3. 添加极端噪声（尖峰噪声）

**原理：**

除了背景高斯噪声外，真实的拉曼光谱有时还会出现幅度较大的突发噪声，表现为尖锐的峰或谷，即尖峰噪声。

**实现步骤：**

1.  **概率性添加：** 以一定的概率 (`extreme_noise_prob`) 决定是否为当前信号样本添加极端噪声。
2.  **生成尖峰参数：** 如果决定添加极端噪声，则随机生成 1 到 3 个尖峰。对于每个尖峰，随机生成其宽度、起始位置和幅度。尖峰幅度通常设置为高斯噪声标准差的数倍。
3.  **确定尖峰方向：** 随机决定每个尖峰是正向（增加信号强度）还是负向（降低信号强度）。
4.  **叠加尖峰：** 将生成的尖峰信号 $S_{spike}[t]$ 叠加到已经包含高斯噪声的信号上，得到最终的带噪信号 $S_{noisy}[t]$：

    $$
    S_{noisy}[t] = S_{noisy\_gaussian}[t] + S_{spike}[t]
    $$

    其中，$S_{spike}[t]$ 在尖峰持续的时间范围内具有非零的幅度。






------------------------------



# 1DCNN
## 1. 网络结构

该 1D CNN 模型的核心结构可以概括如下：

1.  **输入层：** 接收形状为 $(N, 1, L)$ 的带噪拉曼光谱信号。
    * $N$: 批处理大小
    * $1$: 单通道（光谱强度）
    * $L$: 光谱长度（波数点数）

2.  **卷积层序列：**
    * **初始卷积层：** `Conv1d(1, 64, kernel_size=3, padding=1)`
        * 输入通道：1
        * 输出通道：64 (提取 64 个不同的特征)
        * 卷积核大小：3 (关注局部 3 个数据点的模式)
        * 填充：1 (保持输出长度与输入长度一致)
    * **ReLU 激活函数：** 对卷积输出进行非线性激活 (`ReLU()`).
    * **重复卷积块 (18 次)：** 每个块包含：
        * `Conv1d(64, 64, kernel_size=3, padding=1)` (在 64 个特征通道上进一步提取特征)
        * `ReLU()`
    * **输出卷积层：** `Conv1d(64, 1, kernel_size=3, padding=1)`
        * 输入通道：64
        * 输出通道：1 (恢复为单通道的去噪信号)
        * 卷积核大小：3
        * 填充：1

3.  **输出层：** 输出形状为 $(N, 1, L)$ 的去噪后的拉曼光谱信号.

## 2. 降噪原理

该 1D CNN 实现拉曼光谱降噪的核心原理在于通过学习带噪信号到干净信号的映射关系。其工作方式可以分解为以下几点：

1.  **局部特征提取 (卷积)：** 卷积层通过滑动的滤波器（卷积核）自动提取输入信号的局部特征。不同的卷积核可以学习识别不同的噪声模式和信号结构。较小的卷积核（如这里的 3）侧重于捕获精细的局部变化。

    **数学表示：** 对于一维卷积，输出特征图 $Y$ 的每个点 $i$ 是输入信号 $X$ 在局部窗口内与卷积核 $W$ 的加权和：

    $$
    Y[i] = \sum_{k=0}^{K-1} X[i+k-P] \cdot W[k] + b
    $$

    其中，$K$ 是卷积核大小，$P$ 是填充大小，$b$ 是偏置项。

2.  **非线性变换 (ReLU)：** ReLU 激活函数引入了非线性，这使得网络能够学习复杂的非线性关系，对于区分线性和非线性噪声以及信号成分至关重要。

    **数学表示：**
    $$
    ReLU(x) = \max(0, x)
    $$

3.  **层级特征学习 (深层网络)：** 堆叠多个卷积层允许网络以层级方式学习特征。浅层学习到简单的局部模式（可能与噪声或信号的基频相关），而深层则组合这些局部模式，学习到更抽象、更全局的特征，这些高级特征能够更好地区分信号和复杂的噪声。

4.  **映射学习 (训练过程)：** 通过训练，网络学习调整卷积核的权重和偏置，以最小化模型预测的去噪信号与真实的干净信号之间的差异（通常使用均方误差 MSE 作为损失函数）。这个过程使得网络能够建立一个从各种噪声输入模式到相应干净信号输出的有效映射。

5.  **通道数变化：**
    * 初始卷积层将单通道输入扩展到多个通道 (64)，以便提取多种不同的特征。
    * 中间的卷积层在这些多通道特征图上继续提取和细化特征。
    * 最终的卷积层将通道数降回 1，生成最终的单通道去噪信号。


-------------------------------------------------------


# RRCDNet

## 1. 网络结构

RRCDNet 是一种双分支残差卷积去噪网络，其核心结构包含两个并行的卷积神经网络分支，分别处理输入信号，并通过残差学习的方式进行去噪。

1.  **输入层：** 接收形状为 $(N, 1, L)$ 的带噪拉曼光谱信号。
    * $N$: 批处理大小
    * $1$: 单通道（光谱强度）
    * $L$: 光谱长度（波数点数）

2.  **右网络 (标准 CNN 分支)：**
    * 包含一系列标准的 1D 卷积层 (`Conv1d`)、批量归一化层 (`BatchNorm1d`) 和 ReLU 激活函数 (`ReLU()`).
    * **结构：** `Conv1d(1, 64, 3, padding=1)` -> `BatchNorm1d(64)` -> `ReLU()` -> [15 个 `Conv1d(64, 64, 3, padding=1)` -> `BatchNorm1d(64)` -> `ReLU()` 块] -> `Conv1d(64, 1, 3, padding=1)`.
    * 该分支旨在通过标准的卷积操作提取局部和全局的特征。

3.  **左网络 (扩张卷积分支)：**
    * 包含一系列扩张卷积层 (`Conv1d` with `dilation > 1`) 和 ReLU 激活函数。
    * **结构：** `Conv1d(1, 64, 3, padding=1)` -> `BatchNorm1d(64)` -> `ReLU()` -> [7 个 `Conv1d(64, 64, 3, dilation=2, padding=2)` -> `ReLU()` 块] -> `Conv1d(64, 64, 3, padding=1)` -> `BatchNorm1d(64)` -> `ReLU()` -> [6 个 `Conv1d(64, 64, 3, dilation=2, padding=2)` -> `ReLU()` 块] -> `Conv1d(64, 1, 3, padding=1)`.
    * 扩张卷积允许网络在不增加参数数量的情况下增大感受野，捕获更长距离的依赖关系。

4.  **残差学习：**
    * 网络的最终输出通过残差学习实现：`output = x - (right_out + left_out) / 2`。
    * 这里，$x$ 是输入（带噪信号），`right_out` 和 `left_out` 分别是右网络和左网络的输出。模型学习预测噪声残差，并将其从带噪信号中减去，从而得到去噪后的信号。

5.  **权重初始化：**
    * 使用 Kaiming 初始化方法 (`init.kaiming_normal_`) 初始化卷积层权重，有助于加速收敛并提高训练稳定性。
    * 批量归一化层的权重初始化为 1，偏置初始化为 0。

## 2. 降噪原理

RRCDNet 的降噪原理基于以下思想：

1.  **多尺度特征提取：** 通过并行的标准卷积和扩张卷积分支，网络能够捕获不同尺度的特征。标准卷积擅长提取局部细节和短距离依赖，而扩张卷积能够捕获更长距离的上下文信息，这对于处理光谱中可能存在的宽谱特征和全局噪声模式非常有用。

2.  **增大感受野 (扩张卷积)：** 扩张卷积通过在卷积核的元素之间插入空隙来增大感受野，而无需增加卷积核的大小或参数数量。这使得网络能够看到更广范围的输入信息，有助于理解噪声的全局特性以及信号的长程相关性。

    **数学表示 (扩张卷积)：** 扩张率为 $d$ 的一维卷积在位置 $i$ 的输出 $Y[i]$ 可以表示为：

    $$
    Y[i] = \sum_{k=0}^{K-1} X[i + d \cdot (k - P)] \cdot W[k] + b
    $$

    其中，$d$ 是扩张率，$K$ 是卷积核大小，$P$ 是有效填充大小。

3.  **批量归一化：** `BatchNorm1d` 层通过规范化每一层的输出，有助于加速训练，提高模型的鲁棒性，并减少内部协变量偏移。

4.  **残差学习：** 模型学习预测噪声残差（带噪信号和干净信号之间的差异），而不是直接预测干净信号。这种方法简化了学习目标，尤其是在噪声水平较高时，网络更容易学习到需要去除的噪声成分。通过将预测的残差从输入中减去，可以得到去噪后的信号。

## 3. 相对于普通网络的优点

* **多尺度特征融合：** 并行的标准卷积和扩张卷积分支可以提取不同尺度的特征，并最终通过平均融合，使得模型能够更好地理解信号和噪声的复杂结构。
* **更大的感受野：** 扩张卷积有效地增大了网络的感受野，使其能够捕获光谱中的长距离依赖关系，这对于处理基线漂移等全局噪声非常有利。
* **更有效的去噪：** 结合多尺度特征提取和残差学习，RRCDNet 有望实现更有效的噪声去除，尤其是在处理复杂噪声场景时。
* **更快的收敛和更稳定的训练：** 批量归一化的使用有助于网络的训练过程更加稳定和快速。
* **良好的初始化：** Kaiming 初始化有助于避免梯度消失或爆炸问题，加速网络收敛。


-------------------------------------------------------------------------


# DSDN
## 1. 网络结构

DSDN 是一种基于下采样和残差学习的全卷积神经网络，其核心结构包含下采样层、卷积层和多个 ResNet 残差块。

1.  **输入层：** 接收形状为 $(N, 1, L)$ 的带噪拉曼光谱信号。
    * $N$: 批处理大小
    * $1$: 单通道（光谱强度）
    * $L$: 光谱长度（波数点数）

2.  **下采样层 (`DownSampling`):**
    * 包含一个卷积层 (`Conv1d(1, 64, kernel_size=3, stride=1, padding=1)`) 和一个 ReLU 激活函数 (`ReLU()`).
    * **目的：** 将输入通道数从 1 增加到 64，并提取初步特征。虽然名称包含 "DownSampling"，但在该实现中，由于 `stride=1`，信号的长度并没有减小。这里的下采样更像是特征维度的提升。

3.  **全卷积部分：**
    * 包含两个连续的卷积层和 ReLU 激活函数。
    * **结构：** `Conv1d(64, 64, kernel_size=3, padding=1)` -> `ReLU()` -> `Conv1d(64, 64, kernel_size=3, padding=1)` -> `ReLU()`.
    * **目的：** 进一步提取和处理经过下采样后的特征。

4.  **ResNet 部分 (`res_blocks`):**
    * 由多个 (`num_res_blocks`, 默认为 15) `ResidualBlock` 组成，这些残差块顺序连接。
    * **`ResidualBlock` 结构：**
        * `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> `ReLU()` -> `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> **+ (残差连接)** -> `ReLU()`.
    * **目的：** 利用残差连接缓解深层网络中的梯度消失问题，并允许网络学习更复杂的特征映射。残差连接将输入直接添加到卷积块的输出中。

    **数学表示 (ResidualBlock)：**
    令输入为 $X$，残差块的输出 $Y$ 为：
    $$
    Y = ReLU(BatchNorm(Conv(ReLU(BatchNorm(Conv(X)))) + X)
    $$

5.  **输出层 (`conv_out`):**
    * 包含一个卷积层 (`Conv1d(64, 1, kernel_size=3, padding=1)`).
    * **目的：** 将特征通道数从 64 降回 1，生成最终的去噪光谱。

## 2. 降噪原理

DSDN 实现拉曼光谱降噪的核心原理在于：

1.  **特征提取与增强：** 初始的下采样层和全卷积层用于提取输入带噪信号的初步特征，并将特征维度提升到更高的空间 (64 个通道)，以便后续处理。

2.  **深层残差学习：** 通过堆叠大量的 ResNet 残差块，网络能够学习非常复杂的非线性映射，从而有效地去除噪声。残差连接允许信息直接跳过多个卷积层，有助于训练更深的网络并保留重要的低频信息，这对于恢复光谱的整体形状至关重要。模型学习的是输入和目标输出之间的残差（即需要去除的噪声成分）。

3.  **全局信息处理：** 虽然卷积核大小有限，但通过堆叠大量的卷积层（包括残差块中的卷积层），网络可以逐渐扩大其有效感受野，从而处理光谱中的长程依赖关系和全局噪声模式。

4.  **端到端学习：** DSDN 是一个端到端的可训练模型。通过在大量带噪和干净的拉曼光谱数据上进行训练，网络自动学习最优的滤波器权重，以实现最佳的降噪效果。损失函数（通常是均方误差 MSE）指导网络学习预测与真实干净信号尽可能接近的输出。

## 3. 优点

* **深层网络与残差学习：** ResNet 结构允许构建非常深的网络而不会出现严重的梯度消失问题，从而能够学习更复杂的噪声模式和信号结构。
* **有效的信息传递：** 残差连接确保了信息在网络中的顺畅流动，有助于保留重要的信号细节。
* **全卷积结构：** 全卷积网络可以处理任意长度的输入光谱，并且输出与输入具有相同的长度。
* **端到端优化：** 模型可以直接从带噪数据学习到干净数据的映射，无需手动设计特征。
* **潜在的鲁棒性：** 深层网络具有学习各种复杂噪声模式的潜力，因此可能对不同类型的噪声具有较好的鲁棒性。


--------------------------------------


# ADSDN
## 1. 网络结构

ADSDN 在 DSDN 的基础上引入了 CBAM (Convolutional Block Attention Module) 注意力机制，以提升网络的特征表示能力。其核心结构包含下采样层（带有 CBAM）、卷积层（带有 CBAM 的残差块）和输出层。

1.  **输入层：** 接收形状为 $(N, 1, L)$ 的带噪拉曼光谱信号。
    * $N$: 批处理大小
    * $1$: 单通道（光谱强度）
    * $L$: 光谱长度（波数点数）

2.  **下采样层 (`DownSampling`):**
    * 包含一个卷积层 (`Conv1d(1, 64, kernel_size=3, stride=1, padding=1)`)、ReLU 激活函数 (`ReLU()`) 和一个 CBAM 模块 (`CBAM(64)`).
    * **目的：** 将输入通道数增加到 64，提取初步特征，并通过 CBAM 对这些特征进行通道和空间上的加权。

3.  **全卷积部分：**
    * 包含两个连续的卷积层和 ReLU 激活函数。
    * **结构：** `Conv1d(64, 64, kernel_size=3, padding=1)` -> `ReLU()` -> `Conv1d(64, 64, kernel_size=3, padding=1)` -> `ReLU()` -> `CBAM(64)`.
    * **目的：** 进一步提取和处理特征，并利用 CBAM 增强重要特征，抑制不重要特征。

4.  **ResNet 部分 (`res_blocks`):**
    * 由多个 (`num_res_blocks`, 默认为 15) `ResidualBlock` 组成，这些残差块顺序连接。
    * **`ResidualBlock` 结构 (带有 CBAM)：**
        * `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> `ReLU()` -> `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> `CBAM(channels)` -> **+ (残差连接)** -> `ReLU()`.
    * **目的：** 利用残差连接学习更复杂的特征映射，并通过 CBAM 在每个残差块的输出中自适应地调整通道和空间上的特征权重。

5.  **输出层 (`conv_out`):**
    * 包含一个卷积层 (`Conv1d(64, 1, kernel_size=3, padding=1)`).
    * **目的：** 将特征通道数降回 1，生成最终的去噪光谱。

## 2. 注意力机制 (CBAM)

CBAM (Convolutional Block Attention Module) 是一种轻量级的通用注意力模块，可以集成到 CNN 中。它包含两个子模块：通道注意力模块 (Channel Attention Module, CAM) 和空间注意力模块 (Spatial Attention Module, SAM)。

### 2.1. 通道注意力模块 (CAM)

CAM 旨在学习每个特征通道的重要性。它通过对输入特征图进行平均池化和最大池化，然后将池化结果通过一个共享的多层感知机 (MLP)，最后将两个结果相加并通过 Sigmoid 激活函数生成通道注意力图。

**数学表示 (CAM)：**
给定输入特征图 $F \in \mathbb{R}^{C \times L}$ (忽略批大小)，通道注意力图 $M_c \in \mathbb{R}^{C \times 1}$ 的计算如下：

$$
M_c(F) = \sigma(MLP(AvgPool(F)) + MLP(MaxPool(F)))
$$

其中，$\sigma$ 是 Sigmoid 函数，$AvgPool$ 和 $MaxPool$ 分别表示沿空间维度的平均池化和最大池化，$MLP$ 包含一个隐藏层。通道注意力图 $M_c$ 中的每个元素表示对应通道的重要性权重。最终的通道加权特征图通过将输入特征图 $F$ 与 $M_c$ 进行逐元素乘积得到。

### 2.2. 空间注意力模块 (SAM)

SAM 旨在学习每个空间位置的重要性。它首先沿通道维度对输入特征图进行平均池化和最大池化，然后将这两个池化结果在通道维度上拼接，并通过一个卷积层和 Sigmoid 激活函数生成空间注意力图。

**数学表示 (SAM)：**
给定通道注意力加权后的特征图 $F' \in \mathbb{R}^{C \times L}$，空间注意力图 $M_s \in \mathbb{R}^{1 \times L}$ 的计算如下：

$$
M_s(F') = \sigma(Conv_{1 \times k}([AvgPool_{channel}(F'); MaxPool_{channel}(F')]))
$$

其中，$\sigma$ 是 Sigmoid 函数，$AvgPool_{channel}$ 和 $MaxPool_{channel}$ 分别表示沿通道维度的平均池化和最大池化，$[;]$ 表示通道维度的拼接，$Conv_{1 \times k}$ 是一个卷积核大小为 $k$ 的一维卷积层。空间注意力图 $M_s$ 中的每个元素表示对应空间位置的重要性权重。最终的空间加权特征图通过将输入特征图 $F'$ 与 $M_s$ 进行逐元素乘积得到。

### 2.3. CBAM 的集成

在 ADSDN 中，CBAM 模块被集成到下采样层和每个残差块中，对提取的特征在通道和空间维度上进行自适应的加权，使得网络更加关注重要的特征区域和通道，从而提升特征表示能力和降噪性能。

## 3. 降噪原理与优点

ADSDN 的降噪原理与 DSDN 类似，都依赖于深层残差学习来建立带噪信号到干净信号的复杂映射。然而，ADSDN 的关键改进在于引入了 CBAM 注意力机制，这带来了以下优点：

* **特征增强：** CBAM 能够自适应地学习不同通道和空间位置的重要性，增强对降噪任务更关键的特征，抑制不重要的特征，从而提高特征的判别能力。
* **更好的噪声抑制：** 通过关注与信号更相关的特征区域和通道，网络可以更有效地识别和抑制噪声，尤其是一些与信号特征不相关的噪声模式。
* **提升网络性能：** 注意力机制的引入通常可以提升网络的整体性能，使其能够学习到更鲁棒和更有效的特征表示，从而提高降噪效果。
* **轻量级：** CBAM 是一个相对轻量级的模块，引入的额外参数和计算量Compared to the performance gain, it is often acceptable.



---------------------------------------



# PIDN
## 1. 网络结构

PIDN 是在 DSDN 的基础上进行改进的网络，旨在通过引入物理约束来提升拉曼光谱的降噪性能。其核心结构如下：

1.  **输入层：** 接收形状为 $(N, 1, L)$ 的带噪拉曼光谱信号。
    * $N$: 批处理大小
    * $1$: 单通道（光谱强度）
    * $L$: 光谱长度（波数点数）

2.  **下采样层：**
    * 包含一个卷积层 (`Conv1d(1, 64, kernel_size=3, padding=1)`) 和一个 ReLU 激活函数 (`ReLU()`).
    * **改进 (相对于 DSDN/ADSDN):** 与 DSDN 类似，但没有直接集成 CBAM。

3.  **残差块部分：**
    * 由多个 (`num_res_blocks`, 默认为 15) 改进的残差块顺序连接。
    * **改进 (相对于 DSDN/ADSDN):** 每个残差块在两个卷积层后都加入了 **批量归一化层 (`BatchNorm1d`)**。
    * **`ResidualBlock` 结构 (PIDN):**
        * `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> `ReLU()` -> `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> **+ (残差连接)**.

4.  **输出层：**
    * 包含一个卷积层 (`Conv1d(64, 1, kernel_size=3, padding=1)`) 后接一个 **Sigmoid 激活函数 (`Sigmoid()`)**。
    * **改进 (相对于 DSDN/ADSDN):** 引入 Sigmoid 激活函数，将输出限制在 $[0, 1]$ 范围内，这符合归一化后拉曼光谱强度的物理意义（非负且有上限）。

## 2. 基于物理约束的损失函数 (`PhysicsAwareLoss`)

PIDN 的关键创新在于引入了一个定制的物理约束损失函数，该损失函数不仅考虑了预测光谱与干净光谱之间的均方误差，还加入了额外的惩罚项，以鼓励模型学习符合拉曼光谱物理特性的解。

**损失函数定义：**

$$
\mathcal{L}_{total} = \mathcal{L}_{MSE} + \alpha \mathcal{L}_{smooth} + \beta \mathcal{L}_{peak} + \gamma \mathcal{L}_{noise}
$$

其中：

* **$\mathcal{L}_{MSE}$ (均方误差损失):** 标准的损失函数，衡量预测光谱 $\hat{y}$ 与干净光谱 $y$ 之间的像素级差异。

    $$
    \mathcal{L}_{MSE} = \frac{1}{N \cdot L} \sum_{i=1}^{N} \sum_{j=1}^{L} (\hat{y}_{ij} - y_{ij})^2
    $$

* **$\mathcal{L}_{smooth}$ (平滑性约束):** 惩罚预测光谱中的高频噪声，鼓励生成更平滑的光谱。通过计算预测光谱和干净光谱的一阶差分之间的 L1 范数实现。

    $$
    \mathcal{L}_{smooth} = || \nabla \hat{y} - \nabla y ||_1 = \frac{1}{N \cdot (L-1)} \sum_{i=1}^{N} \sum_{j=1}^{L-1} |(\hat{y}_{i,j+1} - \hat{y}_{ij}) - (y_{i,j+1} - y_{ij})|
    $$

* **$\mathcal{L}_{peak}$ (峰保持约束):** 旨在保护光谱中的重要峰信息。通过关注干净光谱的二阶差分为负的区域（潜在的峰区域），并惩罚预测光谱在这些区域与干净光谱的差异。

    首先计算干净光谱的二阶差分：$\nabla^2 y = y_{i,j+1} - 2y_{ij} + y_{i,j-1}$。
    然后创建一个峰值掩码 $M_{peak}$，当 $\nabla^2 y < 0$ 时，$M_{peak} = 1$，否则 $M_{peak} = 0$。
    峰值保持损失为：

    $$
    \mathcal{L}_{peak} = \frac{1}{N \cdot (L-2)} \sum_{i=1}^{N} \sum_{j=2}^{L-1} |(\hat{y}_{ij} \cdot M_{peak_{ij}}) - (y_{ij} \cdot M_{peak_{ij}})|^2
    $$

* **$\mathcal{L}_{noise}$ (噪声模型约束):** 鼓励模型学习符合特定噪声分布的残差（带噪信号减去干净信号）。这里假设噪声服从混合高斯-泊松分布，并通过惩罚预测噪声方差与真实噪声方差的差异来实现。

    令噪声 $n = x - y$， $n_{pred} = x - \hat{y}$。噪声模型约束旨在使预测噪声的统计特性接近真实噪声的统计特性。代码中使用了泊松噪声的方差稳定性约束：

    $$
    \mathcal{L}_{noise} = \mathbb{E} [ (n_{pred}^2 - Var(n))^2 ] \approx \frac{1}{N \cdot (L-1)} \sum_{i=1}^{N} \sum_{j=1}^{L-1} ((x_{ij} - \hat{y}_{ij})^2 - Var(x_i - y_i))^2
    $$

    其中 $Var(x_i - y_i)$ 是每个样本真实噪声的方差。

$\alpha$, $\beta$, $\gamma$ 是控制各项损失函数权重的超参数。

## 3. 相对于 DSDN 和 ADSDN 的改进与原理

PIDN 相对于 DSDN 和 ADSDN 的主要改进在于：

1.  **更规范化的残差块：** 在残差块中加入了批量归一化层，有助于稳定训练并加速收敛。

2.  **物理约束损失函数：** 这是 PIDN 的核心创新。通过引入平滑性约束、峰保持约束和噪声模型约束，PIDN 不仅仅依赖于像素级的 MSE 损失，还引导模型学习生成更符合拉曼光谱物理特性的去噪结果。
    * **平滑性约束**利用了拉曼光谱通常是连续且平滑的特性。
    * **峰保持约束**强调了光谱中重要峰信息的保留，这对于后续的物质识别至关重要。
    * **噪声模型约束**尝试让模型学习数据中噪声的统计特性，从而更准确地分离信号和噪声。

3.  **输出范围限制：** 输出层使用 Sigmoid 激活函数将预测值限制在 $[0, 1]$ 范围内，这与归一化后的光谱强度范围一致，增加了模型的物理合理性。

**优点：**

* **更符合物理特性：** 通过物理约束损失函数，PIDN 能够生成更平滑、峰信息更完整、噪声特性更合理的去噪光谱。
* **潜在的更高性能：** 引入物理先验知识可以帮助模型更好地理解拉曼光谱的结构和噪声特性，从而可能实现比单纯依赖数据驱动的 DSDN 和 ADSDN 更好的降噪效果。
* **更好的峰值保留：** 显式的峰保持损失项有助于在去噪的同时 сохранять 光谱中的关键峰信息。
* **更稳定的训练：** 批量归一化的引入有助于提高训练的稳定性。
* **输出范围约束：** Sigmoid 输出层确保了预测值的物理意义。





----------------------------------------------




# APIDN
## 1. 网络结构

APIDN 在 PIDN 的基础上引入了 CBAM (Convolutional Block Attention Module) 注意力机制，以增强网络的特征表示能力。其核心结构如下：

1.  **输入层：** 接收形状为 $(N, 1, L)$ 的带噪拉曼光谱信号。
    * $N$: 批处理大小
    * $1$: 单通道（光谱强度）
    * $L$: 光谱长度（波数点数）

2.  **初始卷积层：**
    * 包含一个卷积层 (`Conv1d(1, 64, kernel_size=3, padding=1)`) 和一个 ReLU 激活函数 (`ReLU()`).
    * **目的：** 将输入通道数增加到 64，提取初步特征。

3.  **残差块与 CBAM 注意力：**
    * 由多个 (`num_res_blocks`, 默认为 15) 改进的残差块组成，每个残差块后都集成了一个 CBAM 注意力模块。
    * **`ResidualBlock` 结构 (APIDN):**
        * `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> `ReLU()` -> `Conv1d(channels, channels, kernel_size=3, padding=1)` -> `BatchNorm1d(channels)` -> **`CBAMBlock(channels)`** -> **+ (残差连接)**.
    * **`CBAMBlock` 结构:** 包含通道注意力 (`ChannelAttention`) 和空间注意力 (`SpatialAttention`) 两个子模块，用于自适应地调整特征的重要性。

4.  **输出层：**
    * 包含一个卷积层 (`Conv1d(64, 1, kernel_size=3, padding=1)`) 后接一个 **Sigmoid 激活函数 (`Sigmoid()`)**。
    * **目的：** 将特征通道数降回 1，生成最终的去噪光谱，并将输出限制在 $[0, 1]$ 范围内。

## 2. 注意力机制 (CBAM)

CBAM (Convolutional Block Attention Module) 是一种轻量级的通用注意力模块，用于增强 CNN 的特征表示。它依次通过通道注意力模块 (CAM) 和空间注意力模块 (SAM)。

### 2.1. 通道注意力模块 (CAM)

CAM 旨在学习每个特征通道的重要性。它通过对输入特征图进行平均池化和最大池化，然后将池化结果通过一个共享的多层感知机 (MLP)，最后将两个结果相加并通过 Sigmoid 激活函数生成通道注意力图。

**数学表示 (CAM)：**
给定输入特征图 $F \in \mathbb{R}^{C \times L}$，通道注意力图 $M_c \in \mathbb{R}^{C \times 1}$ 的计算如下：

$$
M_c(F) = \sigma(MLP(AvgPool(F)) + MLP(MaxPool(F)))
$$

通道注意力图 $M_c$ 中的每个元素表示对应通道的重要性权重，用于对输入特征图的每个通道进行加权。

### 2.2. 空间注意力模块 (SAM)

SAM 旨在学习每个空间位置的重要性。它首先沿通道维度对输入特征图进行平均池化和最大池化，然后将这两个池化结果在通道维度上拼接，并通过一个卷积层和 Sigmoid 激活函数生成空间注意力图。

**数学表示 (SAM)：**
给定通道注意力加权后的特征图 $F' \in \mathbb{R}^{C \times L}$，空间注意力图 $M_s \in \mathbb{R}^{1 \times L}$ 的计算如下：

$$
M_s(F') = \sigma(Conv_{1 \times k}([AvgPool_{channel}(F'); MaxPool_{channel}(F')]))
$$

空间注意力图 $M_s$ 中的每个元素表示对应空间位置的重要性权重，用于对通道注意力加权后的特征图的每个空间位置进行加权。

### 2.3. CBAM 的集成

在 APIDN 中，CBAM 模块被集成到每个残差块的末尾。这样，在每个残差连接之前，网络能够自适应地学习哪些通道和哪些空间位置的特征对于去噪更为重要，并对其进行增强，抑制不重要的特征。

## 3. 基本原理

APIDN 的降噪原理基于以下几个关键方面：

1.  **深层残差学习：** 通过堆叠多个残差块，网络能够学习复杂的非线性映射，从而有效地去除噪声并恢复干净信号。残差连接有助于缓解梯度消失问题，使得更深的网络能够被有效训练。

2.  **注意力机制引导：** CBAM 注意力模块的引入使得网络能够动态地关注输入特征中信息量更丰富的通道和空间区域。通过对特征进行加权，网络可以更好地利用关键信息进行去噪，并抑制噪声的影响。

3.  **物理约束损失函数 (在训练中使用)：** 虽然网络结构本身没有直接体现物理约束，但 APIDN 在训练过程中使用了 `PhysicsAwareLoss` 损失函数（与 PIDN 相同）。这个损失函数除了标准的均方误差损失外，还包含了平滑性约束、峰保持约束和噪声模型约束，引导网络学习符合拉曼光谱物理特性的解。

4.  **输出范围限制：** 输出层的 Sigmoid 激活函数将预测的光谱强度限制在 $[0, 1]$ 范围内，这符合归一化拉曼光谱的物理意义。

## 4. 优点

* **更强的特征表示能力：** CBAM 注意力机制能够自适应地增强重要特征，抑制不重要特征，从而提高网络的特征表示能力。
* **更好的噪声抑制：** 通过关注关键特征，网络可以更有效地识别和去除噪声，尤其是一些与信号特征不相关的噪声。
* **潜在的更高降噪性能：** 结合注意力机制和物理约束损失，APIDN 有望实现比单独使用物理约束或简单 CNN 更优的降噪效果。
* **更好的峰值保留和光谱平滑性：** 物理约束损失函数中的峰保持和平滑性约束有助于生成更符合拉曼光谱物理特性的去噪结果。
* **稳定的训练和物理意义的输出：** 批量归一化和 Sigmoid 输出层分别有助于训练的稳定性和输出的物理合理性。




------------------------




# 统一评测
## 1. 测试指标

为了全面评估去噪模型的性能，我们采用以下几种常用的测试指标。

### 1.1 均方误差 (Mean Squared Error, MSE)

均方误差衡量的是预测信号与真实信号之间差异的平方的平均值。MSE 越小，表示模型的预测结果与真实值越接近，性能越好。

**数学公式：**

$$
MSE = \frac{1}{N \cdot L} \sum_{i=1}^{N} \sum_{j=1}^{L} (\hat{y}_{ij} - y_{ij})^2
$$

其中：
* $N$ 是样本数量。
* $L$ 是每个信号的长度。
* $\hat{y}_{ij}$ 是第 $i$ 个样本的第 $j$ 个点的预测值。
* $y_{ij}$ 是第 $i$ 个样本的第 $j$ 个点的真实值。

### 1.2 结构相似性指数 (Structural Similarity Index Measure, SSIM)

结构相似性指数是一种感知度量，用于衡量两幅图像或信号的结构相似性。SSIM 的取值范围为 $[-1, 1]$，值越接近 1，表示预测信号与真实信号的结构越相似，感知质量越好。SSIM 考虑了亮度、对比度和结构三个方面的相似性。

**数学公式：**

$$
SSIM(x, y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)}
$$

其中：
* $\mu_x$ 和 $\mu_y$ 分别是信号 $x$ 和 $y$ 的均值。
* $\sigma_x^2$ 和 $\sigma_y^2$ 分别是信号 $x$ 和 $y$ 的方差。
* $\sigma_{xy}$ 是信号 $x$ 和 $y$ 的协方差。
* $c_1$ 和 $c_2$ 是用于稳定除法的常数。

对于一维信号，可以将 SSIM 的概念进行推广，计算局部窗口内的均值、方差和协方差。

### 1.3 平滑度 (Smoothness)

平滑度衡量的是信号的波动程度。对于去噪后的拉曼光谱，我们期望信号在去除噪声的同时保持其原有的平滑性。这里使用一阶差分的绝对值的平均值来衡量平滑度。平滑度越小，信号越平滑。

**数学公式：**

$$
Smoothness(x) = \frac{1}{L-1} \sum_{j=1}^{L-1} |x_{j+1} - x_j|
$$

其中：
* $x$ 是一个信号。
* $L$ 是信号的长度。

### 1.4 峰峰值 (Peak-to-Peak, P2P)

峰峰值衡量的是信号的最大值与最小值之间的差。在去噪过程中，我们希望模型能够保留原始信号的动态范围，因此去噪后信号的峰峰值应该与干净信号的峰峰值接近。

**数学公式：**

$$
Peak2Peak(x) = \max(x) - \min(x)
$$

其中：
* $x$ 是一个信号。

## 2. 测试流程

模型测试的基本流程如下：

1.  **加载测试数据：** 使用 `load_test_data()` 函数加载测试数据集（包含带噪信号和干净信号）。

2.  **加载模型：** 使用 `load_denoising_model(model_path)` 函数加载预训练的去噪模型，并使用 `model.eval()` 设置为评估模式。

3.  **进行预测：** 遍历加载的测试数据，对每个带噪信号使用 `model.predict(noisy_signal)` 函数进行去噪预测，并将结果存储。

4.  **计算指标：** 对比模型的预测结果和真实的干净信号，使用 `compute_mse(predicted, ground_truth)`，`compute_ssim(predicted, ground_truth)`，`compute_smoothness(predicted)`，和 `compute_peak_to_peak(predicted)` 等函数计算整个测试集的平均评估指标。

5.  **可视化结果（可选）：** 使用 `visualize_sample(noisy, clean, denoised, sample_index, save_directory)` 函数选择部分测试样本，将原始带噪信号、干净信号和模型的去噪结果可视化并保存。

6.  **保存结果：** 使用 `save_evaluation_metrics(metrics, save_directory)` 函数将计算得到的平均评估指标保存到指定的文件中。

## 3. 测试方法

在进行测试时，需要注意以下几点：

1.  **数据预处理一致性：** 确保测试数据的预处理方式与训练数据完全一致。

2.  **批处理大小：** 评估时通常使用较小的批处理大小或逐个样本处理，以便于指标计算和可视化。

3.  **禁用梯度计算：** 在评估阶段，务必使用 `with torch.no_grad():` 上下文管理器来禁用梯度计算，以节省内存并加速评估。

4.  **指标实现的准确性：** 确认所使用的评估指标函数（如 SSIM 的一维实现）是正确且适用于当前数据类型的。

5.  **统计意义的评估：** 报告整个测试集上的平均指标，以获得对模型泛化性能的可靠评估。

6.  **定性与定量结合：** 结合数值指标和可视化结果进行分析，更全面地理解模型的优缺点。

7.  **结果记录与比较：** 详细记录每次测试的指标和可视化结果，方便不同模型或同一模型不同版本的性能比较。
